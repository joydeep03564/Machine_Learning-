{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "atmospheric-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "satellite-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are importing the dataset using pandas \n",
    "\n",
    "data=pd.read_csv(\"//home/jyodeep/NLP/Document_classification_project_BBC_Data/CSV_Data_transformer/bbc-text1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "mighty-continent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "motivated-laptop",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "official-maker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category    0\n",
       "text        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "extended-blowing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category    0\n",
       "text        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "warming-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_datapoints = data.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "packed-aircraft",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    2126\n",
       "True       99\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_datapoints.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "artificial-filter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport            511\n",
       "business         510\n",
       "politics         417\n",
       "tech             401\n",
       "entertainment    386\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "completed-george",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 2)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "civil-creation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sport            511\n",
       "business         510\n",
       "politics         417\n",
       "tech             401\n",
       "entertainment    386\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "broke-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "understanding-coordinate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    tv future in the hands of viewers with home th...\n",
       "1    worldcom boss  left books alone  former worldc...\n",
       "2    tigers wary of farrell  gamble  leicester say ...\n",
       "3    yeading face newcastle in fa cup premiership s...\n",
       "4    ocean s twelve raids box office ocean s twelve...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "balanced-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "sound-washer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             tech\n",
       "1         business\n",
       "2            sport\n",
       "3            sport\n",
       "4    entertainment\n",
       "Name: category, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "behavioral-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to decontract the contracted words\n",
    "\n",
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "sacred-jaguar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_urls (vTEXT):\n",
    "    vTEXT = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
    "    return(vTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "detailed-skill",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords= set([ 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "alternate-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "complimentary-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_x=[]\n",
    "for sentence in data['text'].values:\n",
    "  sentence=decontracted(sentence) # expanding the text\n",
    "  sentence=re.sub(r\"http\\S+\", \" \", sentence) # Removing the url\n",
    "  sentence=remove_urls(sentence)\n",
    "  sentence = BeautifulSoup(sentence,'lxml').get_text()\n",
    "  sentence=re.sub('[^A-Za-z0-9]+', ' ', sentence) \n",
    "  #sentence=re.sub(r\"http\\S+\", \" \", sentence)\n",
    "  sentence= re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()\n",
    "  #sentence = BeautifulSoup(sentence,'lxml').get_text()\n",
    "  sentence=' '.join(e.lower() for e in sentence.split() if e.lower() not in stopwords)\n",
    "  #print(sentence)\n",
    "  preprocessed_x.append(sentence.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "rough-geography",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tv future hands viewers home theatre systems plasma high definition tvs digital video recorders moving living room way people watch tv radically different five years time according expert panel gathered annual consumer electronics show las vegas discuss new technologies impact one favourite pastimes us leading trend programmes content delivered viewers via home networks cable satellite telecoms companies broadband service providers front rooms portable devices one talked technologies ces digital personal video recorders dvr pvr set top boxes like us tivo uk sky system allow people record store play pause forward wind tv programmes want essentially technology allows much personalised tv also built high definition tv sets big business japan us slower take europe lack high definition programming not people forward wind adverts also forget abiding network channel schedules putting together la carte entertainment us networks cable satellite companies worried means terms advertising revenues well brand identity viewer loyalty channels although us leads technology moment also concern raised europe particularly growing uptake services like sky happens today see nine months years time uk adam hume bbc broadcast futurologist told bbc news website likes bbc no issues lost advertising revenue yet pressing issue moment commercial uk broadcasters brand loyalty important everyone talking content brands rather network brands said tim hanlon brand communications firm starcom mediavest reality broadband connections anybody producer content added challenge hard promote programme much choice means said stacey jolna senior vice president tv guide tv group way people find content want watch simplified tv viewers means networks us terms channels could take leaf google book search engine future instead scheduler help people find want watch kind channel model might work younger ipod generation used taking control gadgets play might not suit everyone panel recognised older generations comfortable familiar schedules channel brands know getting perhaps not want much choice put hands mr hanlon suggested end kids diapers pushing buttons already everything possible available said mr hanlon ultimately consumer tell market want new gadgets technologies showcased ces many enhancing tv watching experience high definition tv sets everywhere many new models lcd liquid crystal display tvs launched dvr capability built instead external boxes one example launched show humax inch lcd tv hour tivo dvr dvd recorder one us biggest satellite tv companies directtv even launched branded dvr show hours recording capability instant replay search function set pause rewind tv hours microsoft chief bill gates announced pre show keynote speech partnership tivo called tivotogo means people play recorded programmes windows pcs mobile devices reflect increasing trend freeing multimedia people watch want want'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "demographic-station",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'worldcom boss left books alone former worldcom boss bernie ebbers accused overseeing fraud never made accounting decisions witness told jurors david myers made comments questioning defence lawyers arguing mr ebbers not responsible worldcom problems phone company collapsed prosecutors claim losses hidden protect firm shares mr myers already pleaded guilty fraud assisting prosecutors monday defence lawyer reid weingarten tried distance client allegations cross examination asked mr myers ever knew mr ebbers make accounting decision not aware mr myers replied ever know mr ebbers make accounting entry worldcom books mr weingarten pressed no replied witness mr myers admitted ordered false accounting entries request former worldcom chief financial officer scott sullivan defence lawyers trying paint mr sullivan admitted fraud testify later trial mastermind behind worldcom accounting house cards mr ebbers team meanwhile looking portray affable boss admission pe graduate economist whatever abilities mr ebbers transformed worldcom relative unknown telecoms giant investor darling late worldcom problems mounted however competition increased telecoms boom petered firm finally collapsed shareholders lost workers lost jobs mr ebbers trial expected last two months found guilty former ceo faces substantial jail sentence firmly declared innocence'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "million-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=preprocessed_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-equation",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1Jhx44Zl2K51WJ81lxAvuRufoMy28SAG4?usp=sharing#scrollTo=c8PeFWzPZLW_\n",
    "https://colab.research.google.com/drive/1hUsTvUIDs78wzRLAcSHj-x1H5xST4yCc?usp=sharing#scrollTo=9LhzBBgSC3S5\n",
    "\n",
    "About tokenisation and padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-integrity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "defensive-afghanistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "rural-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "\n",
    "max_length = 200\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "training_portion = .8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "covered-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training and validation set \n",
    "training_size=int(len(x)*training_portion)\n",
    "x_train = x[:training_size]\n",
    "y_train = y[:training_size]\n",
    "x_val = x[training_size:]\n",
    "y_val = y[training_size:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "first-density",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1780\n",
      "1780\n",
      "1780\n",
      "445\n",
      "445\n"
     ]
    }
   ],
   "source": [
    "print(training_size)\n",
    "print(len(x_train))\n",
    "print(len(y_train))\n",
    "print(len(x_val))\n",
    "print(len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "agreed-accessory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83, 155, 1092, 1153, 47, 1058, 688, 4957, 74, 1011, 4055, 129, 162, 3920, 1273, 1241, 1508, 41, 8, 890, 83, 6212, 301, 80, 20, 15, 122, 2965, 1260, 2379, 537, 387, 1207, 62, 2809, 2890, 1660, 9, 838, 700, 11, 893, 18470, 10, 607, 1492, 991, 383, 1891, 1153, 723, 47, 464, 1415, 1999, 1561, 117, 305, 107, 2597, 762, 4958, 1025, 566, 11, 4195, 838, 2435, 129, 323, 162, 3920, 8428, 5517, 38, 63, 3051, 27, 10, 5205, 19, 1322, 127, 419, 8, 120, 1323, 71, 4371, 452, 4959, 83, 991, 76, 6213, 72, 2000, 54, 8429, 83, 7, 1059, 74, 1011, 83, 1863, 130, 140, 388, 10, 2729, 40, 131, 1154, 74, 1011, 4196, 3, 8, 452, 4959, 2966, 7, 2546, 6644, 381, 1034, 8430, 1301, 572, 1966, 14251, 701, 10, 464, 1415, 1999, 117, 1814, 379, 839, 1967, 1529, 37, 1720, 2436, 4753, 5206, 2380, 226, 10, 2489, 72, 763, 7, 1026, 1069, 131, 742, 538, 8431, 118, 27, 1322, 1721, 411, 78, 894, 102, 20, 15, 19, 3225, 18471, 36, 1375, 12026, 24, 36, 86, 333, 2265, 36, 21, 429, 218, 1967, 1302, 313, 5207, 298, 763, 1070, 19, 2490, 1720, 5206, 270, 682, 1110, 383, 1931, 370, 381, 1931, 2, 1242, 12027, 1720, 1753, 60, 18472, 18473, 1696, 305, 1722, 3226, 1134, 383, 42, 801, 245, 2810, 336, 54, 532, 379, 2, 18474, 18475, 622, 1243, 183, 83, 3791, 83, 91, 41, 8, 324, 383, 76, 890, 14252, 83, 1153, 379, 464, 10, 839, 2380, 12, 40, 10446, 795, 507, 372, 1754, 155, 533, 18476, 123, 8, 324, 76, 890, 1135, 1034, 1416, 338, 58, 1932, 1194, 731, 81, 237, 272, 969, 71, 338, 3, 2437, 682, 1260, 2174, 1576, 6645, 3653, 3052, 8430, 1034, 1931, 163, 365, 1471, 3, 76, 54, 532, 99, 1092, 4, 12027, 914, 87, 3053, 18477, 2220, 8432, 98, 714, 401, 406, 2, 4, 12027, 2491, 387, 1417, 43, 76, 9, 969, 838, 6646, 2435, 32, 4056, 83, 1629, 796, 74, 1011, 83, 1863, 7175, 32, 9, 1432, 6647, 6648, 3921, 1389, 4055, 600, 8428, 3654, 1059, 533, 3922, 3051, 11, 853, 600, 62, 18478, 5847, 6647, 83, 946, 5205, 8428, 546, 3923, 11, 10, 223, 1999, 83, 117, 18479, 92, 600, 2891, 8428, 62, 660, 2338, 3654, 4960, 3338, 372, 4561, 38, 4371, 9321, 83, 660, 269, 106, 261, 2660, 316, 608, 62, 3792, 707, 3548, 5205, 143, 18480, 379, 8, 71, 1681, 991, 769, 818, 77, 566, 2967, 1303, 1492, 9322, 1577, 8, 890, 76, 76]\n",
      "419\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size,oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(x)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "print(train_sequences[0])\n",
    "print(len(train_sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "expensive-explorer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1769, 533, 210, 1770, 1277, 112, 1769, 533, 5821, 1546, 639, 5822, 710, 249, 26, 2436, 1205, 4089, 25, 4753, 321, 5823, 26, 744, 4090, 801, 1696, 3912, 4, 1546, 3, 1363, 1769, 335, 170, 44, 4286, 2011, 547, 1734, 3011, 1179, 64, 257, 4, 5823, 95, 2919, 1338, 710, 8331, 2011, 435, 801, 1414, 5440, 8332, 1104, 2368, 3221, 1872, 778, 9365, 388, 4, 5823, 336, 1503, 4, 1546, 23, 2436, 181, 3, 1459, 4, 5823, 3359, 336, 166, 4, 1546, 23, 2436, 1873, 1769, 1770, 4, 8332, 6272, 22, 3359, 4089, 4, 5823, 718, 2660, 1919, 2436, 4287, 3605, 112, 1769, 109, 295, 1397, 1431, 1058, 801, 1696, 461, 7482, 4, 1058, 718, 710, 9366, 229, 517, 16626, 349, 1769, 2436, 244, 642, 4, 1546, 131, 681, 307, 6819, 16627, 533, 5075, 16628, 8333, 1415, 1920, 9367, 4, 1546, 5824, 1769, 6820, 4503, 1544, 606, 3606, 6821, 536, 1769, 335, 5825, 75, 490, 763, 1544, 2587, 12845, 64, 1547, 4286, 1076, 242, 735, 242, 368, 4, 1546, 517, 88, 13, 16, 108, 188, 1338, 112, 7483, 1143, 3360, 1849, 2369, 4504, 2241, 7484]\n"
     ]
    }
   ],
   "source": [
    "print(train_sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "pointed-mexico",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sequences[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "awful-graham",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18181, 988, 90, 20, 206, 264, 2664, 114, 999, 1284, 2, 90, 20, 2718, 58, 26, 473, 18181, 616, 767, 2, 1534, 1683, 2007, 23, 3, 5549, 2021, 65, 9821, 201, 1776, 3375, 1508, 459, 18028, 116, 343, 1088, 2, 9, 11476, 304, 16180, 11474, 507, 2294, 16, 285, 26, 1459, 1284, 293, 5564, 4376, 851, 1902, 1165, 2324, 2, 295, 329, 6891, 1459, 462, 2816, 5, 1698, 4172, 511, 6, 96, 594, 484, 1534, 264, 2664, 3208, 3014, 1384, 59, 1821, 221, 26, 1028, 470, 2570, 26, 46, 9, 200, 1476, 6231, 1991, 1614, 12406, 508, 218, 1302, 14, 70, 1300, 3072, 5523, 10800, 4076, 278, 23, 1311, 508, 69, 69, 1284, 295, 2495, 3432, 26, 3882, 116, 717, 4376, 1165, 2324, 293, 445, 10879, 988, 611, 13386, 8224, 616, 767, 1, 1, 256, 369, 221, 1284, 7, 1558, 186, 26, 473, 7964, 6418, 124, 22, 806, 507, 9885, 17926]\n",
      "151\n"
     ]
    }
   ],
   "source": [
    "validation_sequences = tokenizer.texts_to_sequences(x_val)\n",
    "print(validation_sequences[0])\n",
    "print(len(validation_sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "informational-register",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tokenizer = Tokenizer()\n",
    "label_tokenizer.fit_on_texts(y)\n",
    "\n",
    "training_label_seq = np.array(label_tokenizer.texts_to_sequences(y_train))\n",
    "validation_label_seq = np.array(label_tokenizer.texts_to_sequences(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "overall-baseball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\n",
      "[5]\n"
     ]
    }
   ],
   "source": [
    "print(training_label_seq[0])\n",
    "print(validation_label_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "worldwide-edward",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_x = keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=maxlen)\n",
    "validation_x = keras.preprocessing.sequence.pad_sequences(validation_sequences, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "sunrise-fossil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n",
      "(1780, 200)\n"
     ]
    }
   ],
   "source": [
    "print(training_x[0].shape)\n",
    "print(training_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "funny-springfield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n",
      "(445, 200)\n"
     ]
    }
   ],
   "source": [
    "print(validation_x[0].shape)\n",
    "print(validation_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "deadly-dakota",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1780, 1)\n",
      "(445, 1)\n"
     ]
    }
   ],
   "source": [
    "print(training_label_seq.shape)\n",
    "print(validation_label_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-mother",
   "metadata": {},
   "source": [
    "# implement transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "usual-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "facial-oriental",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "nominated-header",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ethical-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(6, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "brave-length",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "56/56 [==============================] - 6s 87ms/step - loss: 1.6576 - accuracy: 0.2684 - val_loss: 1.2111 - val_accuracy: 0.6764\n",
      "Epoch 2/15\n",
      "56/56 [==============================] - 5s 85ms/step - loss: 0.9712 - accuracy: 0.7049 - val_loss: 0.5257 - val_accuracy: 0.9348\n",
      "Epoch 3/15\n",
      "56/56 [==============================] - 5s 86ms/step - loss: 0.3659 - accuracy: 0.9279 - val_loss: 0.1844 - val_accuracy: 0.9416\n",
      "Epoch 4/15\n",
      "56/56 [==============================] - 5s 85ms/step - loss: 0.0888 - accuracy: 0.9855 - val_loss: 0.1399 - val_accuracy: 0.9528\n",
      "Epoch 5/15\n",
      "56/56 [==============================] - 5s 86ms/step - loss: 0.0300 - accuracy: 0.9998 - val_loss: 0.1358 - val_accuracy: 0.9551\n",
      "Epoch 6/15\n",
      "56/56 [==============================] - 5s 87ms/step - loss: 0.0196 - accuracy: 0.9997 - val_loss: 0.1238 - val_accuracy: 0.9596\n",
      "Epoch 7/15\n",
      "56/56 [==============================] - 5s 87ms/step - loss: 0.0139 - accuracy: 0.9998 - val_loss: 0.1290 - val_accuracy: 0.9640\n",
      "Epoch 8/15\n",
      "56/56 [==============================] - 5s 85ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.1331 - val_accuracy: 0.9596\n",
      "Epoch 9/15\n",
      "56/56 [==============================] - 5s 88ms/step - loss: 0.0079 - accuracy: 0.9999 - val_loss: 0.1466 - val_accuracy: 0.9618\n",
      "Epoch 10/15\n",
      "56/56 [==============================] - 5s 86ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.1524 - val_accuracy: 0.9618\n",
      "Epoch 11/15\n",
      "56/56 [==============================] - 5s 86ms/step - loss: 0.0085 - accuracy: 0.9984 - val_loss: 0.1474 - val_accuracy: 0.9640\n",
      "Epoch 12/15\n",
      "56/56 [==============================] - 5s 85ms/step - loss: 0.0038 - accuracy: 0.9999 - val_loss: 0.1565 - val_accuracy: 0.9640\n",
      "Epoch 13/15\n",
      "56/56 [==============================] - 5s 85ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.1510 - val_accuracy: 0.9640\n",
      "Epoch 14/15\n",
      "56/56 [==============================] - 5s 87ms/step - loss: 0.0037 - accuracy: 0.9998 - val_loss: 0.1515 - val_accuracy: 0.9640\n",
      "Epoch 15/15\n",
      "56/56 [==============================] - 5s 87ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.1499 - val_accuracy: 0.9640\n"
     ]
    }
   ],
   "source": [
    "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "   training_x,training_label_seq, batch_size=32, epochs=15, validation_data=(validation_x,validation_label_seq)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-ethics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-valentine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-round",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-houston",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
